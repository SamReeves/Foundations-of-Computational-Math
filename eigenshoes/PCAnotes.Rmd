---
title: "Principal Component Analysis Notes"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Combination

In PCA, we are looking for a linear combination of variables of the matrix X (assumed correlated) that are

1. uncorrelated with each other (orthogonal) and that
2. maximize a reduction in the order of the variance of the original matrix.

In fact, this is identical to maximizing the $R^2$ in a regression model.

## Y is "latent"

In PCA, we do not know what Y, our new matrix, will be.  It is latent.  We build it solely from making a linear combination of X that satisfies the two conditions above.

## On the Iris Dataset

We typically have a data matrix of $n$ observations on $p$ correlated variables $x_1, x_2, ... x_p$.  PCA looks for a transformation of $x_i$ on $p$ new variables $y_i$ that are uncorrelated.

We parse the variance.

```{r}
set.seed(1337)

data <- iris[,1:4]

# Correlation:
cor(data)
```

\clearpage

We are looking for a transformation of the data matrix $X (n \times p)$ such that:

$Y = a^T$ and $X = a_1X_1 + a_2X_2 + ... + a_pX_p$ 

where $a = (a_1, a_2, ..., a_p)^T$ is a column vector.


New variables $Y_i$ are linear combination of the original variables $x_i: Y_i = a_{i1}x_1 + a_{i2}x_2 + ... + a_{ip}x_p$

The variables $Y_i$ are derived in decreasing order of importance, they are called principal components.

This is sensitive to scale!  Always use it with standardized data!  Use correlation matrix, NOT covariance matrix.

The new variables have a variance equal to their corresponding eigenvalue:

\[Var(Y_i) = \lambda_i\] for all $i = 1, 2, ..., p$

A small $\lambda_i$ is small variance! Data changes little in the $Y_i$ direction.

The relative variance explained by each principal component is:

\[\lambda_i / \Sigma\lambda_i\]

## How many $Y_i$ to keep?

Enough principal components to have a cumulative variance explained by them.  70%? 90%?  It's an art...

Retain those $Y_i$ that have eigenvalues greater than average variance.

Kaiser criterion: keep PCs with eigenvalues > 1

Jollife criterion: keep PCs with eigenvalues > .7

Scree plot: represents the ability of PCs to explain the variation in the original dataset.

## Generating the Principal Components

Maximize the variance of the projection of the observations on the Y variables.  Remember:

\[Var(aX) = a^2Var(X)\]

\[Max_{a,l}L1 = a^T_1Ca_1 - \lambda_1(a^T_1a_1 - 1)\]
\[Max_{a,l}L2 = a^T_2Ca_2 - \lambda_2(a^T_2a_2 - 1) - \lambda_1(a^T_1a_2 - 0)\]

The matrix $C = Var(X)$ is the covariance matrix of the $X_i$ variables.  Remember, the covariance matrix is not standardized, and any analysis using it is NOT invariant to scale.  Using the correlation matrix makes more sense, normally, and the covariance / correlation matrices of standardized variables are... the same!

\clearpage

```{r}
cor(data)
cov(data)
```

```{r}
(myPCA <- princomp(data,
                  scores = TRUE,
                  cor = TRUE))
```

```{r}
(myPCA$scores)
```

```{r}
(myEigen <- eigen(cor(data)))

myEigen$values[1]/sum(myEigen$values)
```

Notice that the eigenvalue is the same as squaring the standard deviation! 

```{r}
(myPCA$loadings)
(myPCA$sdev)
(myPCA$sdev[1] ^ 2)
```

```{r}
myEigen$values / sum(myEigen$values)

sum(myEigen$values[1:2]) / sum(myEigen$values)
```

# Eigenshoes

1. We take all the images of all available shoes, centered and clean.
2. We then recombine the images by weighting pixels and their associated colors.
3. The weighting is done so that the dominant (most important) characteristics (design features) that describe the shoes are placed in the first image.
4. The second image has the second-most dominant features and so on.
5. Each shoe image can be reconstructed from these images by reverse weighting.
6. A small subset of these images can reconstruct the design of the original shoes with high accuracy.

The predominant shoe features emerge with very few images...

Principal components are nothing more than Eigenvectors, at the end of the day.  The Eigenvalues are nothing more than the variance!


```{r}
var(myPCA$scores[,1])
myEigen
```

\[Var(Eigenvectors) = \lambda\]
```{r eval=FALSE}
# Prepare to merge

master <- read.csv("")
master$filename <- master$Profile_2500x1200_JPG
master1$filename <- master1$Profile_2500x1200_JPG

var <- mypca$sdev^2
sum(var[1:150]) / sum(var) # Eigenvalues divided by the variability
myvar <- mypca$scores[1:150,] # Captures most of the variability of the pictures!

mypixels <- data.frame(t(myvar))

mypixels$filename <- files

myfile <- read.csv("")
myfile$filename <- myfile$Profile_2500x1200_JPG


# Merge and save

total2 <- merge(mypixels,
                myfile,
                by = "filename",
                all.x = TRUE)

#rm(mupixels)
#rm(master)
#total$X <- NULL
#write.csv(total, ".csv")


# Analyze best and worst-selling products

setwd("~/focm/wk4")
m1 <- read.csv(".csv")
m2 = read.csv("")

total2 <- rbind(m1, m2)
myagg7 <- aggregate(x~Style.Number, data = total2, FUN = sum)
myagg7 <- myagg7[order(myagg7$x, decreasing = TRUE), ]
mymerge <- merge(myagg7, myfile,
                 by.x = "Style.Number",
                 by.y = "Style.Number", all.y = TRUE)

# TRANSFORM IMAGES

# GLOBALS
height <- 1200
width <- 2500
scale <- 20

newdata <- im
dim(newdata) <- c(length(files), height*width*3/scale^2)

mypca <- princomp(t(as.matrix(newdata)), scores = TRUE, cor = TRUE)

# Generate Eigenshoes

mypca2 <- t(mypca$scores)
dim(mypca2) <- c(length(files),
                 height / scale,
                 width / scale,
                 3)
par(mfrow = c(5,5))
par(mai = c(0.001, 0.001, 0.001, 0.001))
for (i in 1:25) { # Plot first 25 eigenshoes only
  plot_jpeg(writeJPEG(mypca2[i,,,],
                      bg = white))
  }
```

