---
title: "Regression, Part 2"
author: "Sam Reeves"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Who.csv contains real-world data from 2008:


\[\begin{array}{r l}
Country & \text{name of the country} \\
LifeExp & \text{average life expectancy for the country in years} \\
InfantSurvival & \text{proportion of those surviving to one year or more} \\
Under5Survival & \text{proportion of those surviving to five years or more} \\
TBFree & \text{proportion of the population without TB} \\
PropMD & \text{proportion of the population who are MDs} \\
PropRN & \text{proportion of the population who are RNs} \\
PersExp & \text{mean personal expenditures on healthcare in US dollars at average exchange rate} \\
GovtExp & \text{mean government expenditures per capita on healthcare, US dollars at average exchange rate} \\
TotExp & \text{sum of personal and government expenditures}\\
\end{array}\]

---

> 1. Provide a scatterplot of LifeExp~TotExp, and run a simple linear regression.  Do not transform the variables.  Provide and interpret the F statistics, $R^2$, standard error, and p-values only.  Discuss whether the assumptions of simple linear regression are met.

```{r, fig.height=2, fig.width=4, message=FALSE}
library(ggplot2)
library(dplyr)

f <- read.csv("who.csv")
lm1 <- lm(LifeExp ~ TotExp, f)

ggplot(f, aes(x = TotExp, y = LifeExp)) +
  theme_dark() + 
  geom_point() +
  geom_abline(intercept = lm1$coefficients[1],
              slope = lm1$coefficients[2],
              color = "white")
```

The scatterplot doesn't visually suggest a linear relationship.  I have a very strong urge to take the log of the total expenditures, however, we've been instructed not to transform the variables at all.

```{r}
summary(lm1)
anova <- aov(lm1)
summary(anova)
```

Looking first at the p-value, which is really small, we can reject the null hypothesis... That means that the relationship here is not easily explained by regular variance alone.

We don't need to consider the F-statistic.  This model has only one independent variable.

$R^2$ is only about 26%.  That means that our independent variable only accounts for about a quarter of the variability in the y values.  Our model has very little predictive power.

The standard error seems high.  The range of values is from 40 to 80, and with a SE of nearly 10, I would assume that this is not a viable linear model.

We do not pass the tests for linearity.

\clearpage

```{r, fig.height=3, fig.width=3}
plot(lm1)
```

> 2. Raise life expectancy to the 4.6 power (i.e., LifeExp^4.6). Raise total expenditures to the 0.06 power (nearly a log transform, TotExp^.06). Plot LifeExp^4.6 as a function of TotExp^.06, and r re-run the simple regression model using the transformed variables. Provide and interpret the F statistics, R^2, standard error, and p-values. Which model is "better?"

```{r, fig.height=3, fig.width=3}
g <- f %>%
  mutate(LifeExp = LifeExp^4.6,
         TotExp = TotExp^0.06)

lm2 <- lm(LifeExp ~ TotExp, g)
plot(lm2)
```

Holy cow!  Our linear model is suddenly behaving very nicely!

```{r}
summary(lm2)
anova2 <- aov(lm2)
summary(anova2)
```

```{r}
ggplot(g, aes(x = TotExp, y = LifeExp)) +
  theme_dark() + 
  geom_point() +
  geom_abline(intercept = lm2$coefficients[1],
              slope = lm2$coefficients[2],
              color = "white") +
  labs(title = "Transformed Data")
```

\clearpage

> 3. Using the results from 2, forecast life expectancy when $TotExp^.06 = 1.5$. Then forecast life expectancy when TotExp^.06=2.5.

```{r}
forecast <- function(x, lm) {
  intercept <- lm$coefficients[1]
  slope <- lm$coefficients[2]
  
  y <- (slope * x) + intercept
  
  names(y) <- c()
  LE <- y^(1/4.6)
  
  return(LE)
}

forecast(1.5, lm2)
forecast(2.5, lm2)
```

I suppose this is reasonable?

\clearpage

> 4. Build the following multiple regression model and interpret the F Statistics, R^2, standard error, and p-values. How good is the model?

\[LifeExp = b_0 + (b_1 \times PropMD) + (b_2 \times TotExp) + (b_3 \times PropMD \times TotExp)\]

```{r}
lm3 <- lm(LifeExp ~ PropMD + TotExp + (PropMD * TotExp), f)
summary(lm3)
```

Looking again at the p-values first, all are very near zero.  The F-statistic and its p-value suggest that the model is not overly complex, that the combination of independent variables together compose a model for which we can reject the null hypothesis.

```{r}
anova3 <- aov(lm3)
summary(anova3)
```

\clearpage

```{r, fig.height=3, fig.width=3, warning=FALSE}
plot(lm3)
```

These plots visually confirm that we have an appropriate model.

\clearpage

> 5. Forecast $LifeExp$ when $PropMD = 0.03$ and $TotExp = 14$.  Does this forecast seem realistic?  Why or why not?

```{r}
forecast2 <- function(x1, x2, lm) {
  intercept <- lm$coefficients[1]
  m1 <- lm$coefficients[2]
  m2 <- lm$coefficients[3]
  
  y <- intercept + (m1 * x1) + (m2 * x2)
  
  names(y) <- c()
  
  return(y)
}

forecast2(0.03, 14, lm3)
```

Well, unfortunately, this doesn't seem like a reasonable answer.

This is a ridiculously high life expectancy for a couple of reasons.  It falls way outside of the range of ages in the initial dataset, so we can't assume that the relationship we have described in this model goes out to infinity.  It's a really advanced age.  It's much safer to assume that there are diminishing returns on these expenditures past a certain point...  Where to cap the expected values?  I would say that maybe it's possible to arrive at a life expectancy a bit higher than the highest available in the data, but certainly not by more than a few years.