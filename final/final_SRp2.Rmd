---
title: "Foundations of Computational Math -- Final Exam Part 2"
author: "Sam Reeves"
date: "11/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part 2. The Venerable MNIST Dataset

### 1. Getting Started

> Get a Kaggle.com account.

Username: samreeves

### 2. The Dataset

> Download the data for /c/digit-recognizer/overview -- The MNIST dataset of handwritten images.

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(nnet)

mnist_train <- tibble(read.csv('prob2/train.csv'))
mnist_test <- tibble(read.csv('prob2/test.csv'))
```

### 3. Min-Max Scaling (5)

> Using the training.csv file, plot representations of the first 10 images to understand the data format.  Go ahead and divide all pixels by 255 to produce values between 0 and 1.

```{r}
train.t <- cbind(mnist_train[,1],
                 mnist_train[,2:785]/255)
```

### 4. Understanding the Data Part 1 (5)

> What is the frequency distribution of the numbers in the dataset? (5)

```{r}
mnist_train %>% ggplot(aes(x = label)) +
  geom_histogram() +
  labs(title = 'Training Set Frequency')
```

The frequency distribution of the training set is basically uniform.

### 5. Understanding the Data Part 2 (5)

> For each number, provide the mean pixel intensity.  What does this tell you?

```{r}
intensity <- train.t %>%
  group_by(label) %>%
  summarize_all("mean")

rowMeans(intensity[,2:785])
```

This tells you "how much ink" is used to write each number on average.  It makes sense that "0" and "8" would have the greatest average intensity, while "1" would have the lowest.

### 6. Principal Component Analysis (5)

> Reduce the data by using principal components that account for 95% of the variance.  How many components did you generate?  Use PCA to generate all possible components (100% of the variance).  How many components are possible? Why?

```{r}
mnist.pca <- prcomp(train.t[,2:785], scale=FALSE)
```

```{r}
mnist.var <- mnist.pca$sdev^2
mnist.var.percentage <- round(mnist.var/sum(mnist.var) * 100, 1)

barplot(mnist.var.percentage)
```

This is an exponential distribution...

```{r}
head(pexp(mnist.var.percentage), 145)
```


### 7. Plotting and Noise (5)

> Plot the first 10 images generated by PCA.  They will appear to be noise.  Why?

### 8. Accounting for Variance (5)

> Now, select only those images that have labels that are 8's.  Re-run PCA that accounts for all of the variance (100%).  Plot the first 10 images.  What do you see?

### 9. Multinomial Model (10)

> An incorrect approach to predicting the images would be to build a linear regression model with $y$ as the digit values and $X$ as the pixel matrix.  Instead, we can build a multinomial model that classifies the digits.  Build a multinomial model on the entirety of the training set.  Then, provide its classification accuracy (percent correctly identified) as well as a matrix of observed versus forecast values (confusion matrix).  This matrix will be a 10 x 10, and correct classifications will be on the diagonal.

```{r}
model <- multinom(label ~ ., train.t,
                  MaxNWts = 1000000,
                  maxit = 1200)
```


```{r}
#save(model, file="weights.Rdata", envir = parent.frame())
#weights <- load('weights.Rdata')
```

