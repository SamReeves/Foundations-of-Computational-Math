---
title: "Part 2"
author: "Sam Reeves"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(stats)
set.seed(1337)
```


# Part 2. The Venerable MNIST Dataset

## 1. Getting Started

### Get a Kaggle.com account.

Username: samreeves

## 2. The Dataset

### Download the data for /c/digit-recognizer/overview -- The MNIST dataset of handwritten images.

```{r, message=FALSE, warning=FALSE}
mnist_train <- tibble(read.csv('prob2/train.csv'))
mnist_test <- tibble(read.csv('prob2/test.csv'))
```

## 3. Min-Max Scaling (5)

### Using the training.csv file, plot representations of the first 10 images to understand the data format.  Go ahead and divide all pixels by 255 to produce values between 0 and 1.

```{r}
train.transformed <- cbind(mnist_train[,1],
                 mnist_train[,2:785]/255)

for(i in 1:10) {
  im <- matrix(unlist(train.transformed[i,-1]),
              nrow=28, byrow=TRUE)
  image(t(apply(im, 2, rev)))
}
```

## 4. Understanding the Data Part 1 (5)

### What is the frequency distribution of the numbers in the dataset? (5)

```{r}
mnist_train %>% ggplot(aes(x = label)) +
  geom_histogram(binwidth = 0.5) +
  labs(title = 'Training Set Frequency')
```

The frequency distribution of the training set is basically uniform with some natural variance.

## 5. Understanding the Data Part 2 (5)

### For each number, provide the mean pixel intensity.  What does this tell you?

```{r}
intensity <- train.transformed %>%
  group_by(label) %>%
  summarize_all("mean")

rowMeans(intensity[,2:785])
```

This tells you "how much ink" is used to write each number on average.  It makes sense that "0" and "8" would have the greatest average intensity, while "1" would have the lowest.

## 6. Principal Component Analysis (5)

### Reduce the data by using principal components that account for 95% of the variance.  How many components did you generate?  Use PCA to generate all possible components (100% of the variance).  How many components are possible? Why?


```{r}
mnist.pca <- prcomp(train.transformed[,2:785])
mnist.var <- mnist.pca$sdev^2

mnist.var.percentage <- mnist.var/sum(mnist.var) * 100
barplot(mnist.var.percentage)
```

This is an exponential distribution...

```{r}
head(mnist.var.percentage)
sum(mnist.var.percentage[1:136])
```


```{r}
qplot(c(2:785), mnist.var.percentage) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
```


I suppose that there are 136 principal components which account for 95% of the variance in the set.... After 700 or so, we approach 100%.

## 7. Plotting and Noise (5)

### Plot the first 10 images generated by PCA.  They will appear to be noise.  Why?

```{r}
for (i in 1:10) {
  im <- mnist.pca$x %*% t(mnist.pca$rotation) + mnist.pca$center
  image(as.matrix(im[,i], nrow=28, byrow=TRUE))
}
```

```{r}
for(i in 1:10) {
  im <- matrix(unlist(train.transformed[i,-1]),
              nrow=28, byrow=TRUE)
  image(t(apply(im, 2, rev)))
}
```

This is a mixture of all the digits.  There will be no discernible signature of one digit in any of the principal components in the set.

## 8. Accounting for Variance (5)

### Now, select only those images that have labels that are 8's.  Re-run PCA that accounts for all of the variance (100%).  Plot the first 10 images.  What do you see?

```{r}
eights <- train.transformed %>%
  subset(label == 8)

eights.pca <- prcomp(eights[,2:785])

for (i in 1:10) {
  im <- eights.pca$x %*% t(eights.pca$rotation) + eights.pca$center
  image(as.matrix(im[,i]))
}
```

```{r}
autoplot(eights.pca,
         data = eights)
```
I guess I'm still not plotting these correctly, but I also ay point out that there is substantially less disorder in the plots for just the eights than for the plots representing principal components for the entire set.

## 9. Multinomial Model (10)

### An incorrect approach to predicting the images would be to build a linear regression model with $y$ as the digit values and $X$ as the pixel matrix.  Instead, we can build a multinomial model that classifies the digits.  Build a multinomial model on the entirety of the training set.  Then, provide its classification accuracy (percent correctly identified) as well as a matrix of observed versus forecast values (confusion matrix).  This matrix will be a 10 x 10, and correct classifications will be on the diagonal.

```{r}
#model <- multinom(label ~ ., train.transformed,
                  #MaxNWts = 100000,
                  #maxit = 1000)

#test.transformed <- mnist_test[,1:784]/255
#predictions <- predict(model, test.transformed, type ='class')
```

Well.... Because the test set doesn't contain any labels, I simply submitted to kaggle.  My username is "Sam Reeves", and the score I received for the submission is 0.90375.

Due to time and computing constraints, I cannot split the data and train again, and so, I am unable to produce a confusion matrix for these predictions.
